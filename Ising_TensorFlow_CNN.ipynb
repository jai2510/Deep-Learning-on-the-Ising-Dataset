#To train the model on a Convolutional Neural Network, replace the model class, in the DNN code, by the following class, and run the code:

class model(object):
    # build the graph for the CNN
    def __init__(self,N_neurons,opt_kwargs):

        # define global step for checkpointing
        self.global_step=tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')

        self.L=40
        self.n_feats= self.L**2 
        self.n_categories=2

        # create placeholders for input X and label Y
        self.create_placeholders()
        # create weight and bias, initialized to 0 and construct CNN to predict Y from X
        self.create_CNN()
        # define loss function
        self.create_loss()
        # use gradient descent to minimize loss
        self.create_optimiser(opt_kwargs)
        # create accuracy
        self.create_accuracy()


        print("finished creating CNN")

    def create_placeholders(self):
        with tf.name_scope('data'):
            self.X=tf.compat.v1.placeholder(tf.float32, shape=(None,self.n_feats), name="X_data")
            self.Y=tf.compat.v1.placeholder(tf.float32, shape=(None,self.n_categories), name="Y_data")
            self.dropout_keepprob=tf.compat.v1.placeholder(tf.float32, name="keep_probability")


    def create_CNN(self, N_filters=10):
        with tf.name_scope('CNN'):
            # conv layer 1, 5x5 kernel, 1 input 10 output channels
            W_conv1 = self.weight_variable([5, 5, 1, N_filters],name='conv1',dtype=tf.float32) 
            b_conv1 = self.bias_variable([N_filters],name='conv1',dtype=tf.float32)
            X_reshaped = tf.reshape(self.X, [-1, self.L, self.L, 1])
            h_conv1 = tf.nn.relu(self.conv2d(X_reshaped, W_conv1, name='conv1') + b_conv1)

            # Pooling layer - downsamples by 2X.
            h_pool1 = self.max_pool_2x2(h_conv1,name='pool1')
            # conv layer 2, 5x5 kernel, 10 input 20 output channels
            W_conv2 = self.weight_variable([5, 5, 10, 20],name='conv2',dtype=tf.float32)
            b_conv2 = self.bias_variable([20],name='conv2',dtype=tf.float32)
            h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2, name='conv2') + b_conv2)

            # Dropout - controls the complexity of the CNN, prevents co-adaptation of features.
            h_conv2_drop = tf.nn.dropout(h_conv2, self.dropout_keepprob,name='conv2_dropout')

            # Second pooling layer.
            h_pool2 = self.max_pool_2x2(h_conv2_drop,name='pool2')

            # Fully connected layer 1 -- after second round of downsampling, our 40x40 image
            # is down to 7x7x20 feature maps -- maps this to 50 features.
            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*20])

            W_fc1 = self.weight_variable([7*7*20, 50],name='fc1',dtype=tf.float32)
            b_fc1 = self.bias_variable([50],name='fc1',dtype=tf.float32)

            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

            # Dropout - controls the complexity of the CNN, prevents co-adaptation of features.
            h_fc1_drop = tf.nn.dropout(h_fc1, self.dropout_keepprob,name='fc1_dropout')

            # Map the 50 features to 2 classes, one for each phase
            W_fc2 = self.weight_variable([50, self.n_categories],name='fc12',dtype=tf.float32)
            b_fc2 = self.bias_variable([self.n_categories],name='fc12',dtype=tf.float32)

            self.Y_predicted = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

    def weight_variable(self, shape, name='', dtype=tf.float32):
        """weight_variable generates a weight variable of a given shape."""
        initial = tf.random.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial,dtype=dtype,name=name)


    def bias_variable(self, shape, name='', dtype=tf.float32):
        """bias_variable generates a bias variable of a given shape."""
        initial = tf.constant(0.1, shape=shape)
        return tf.Variable(initial,dtype=dtype,name=name)


    def conv2d(self, x, W, name=''):
        """conv2d returns a 2d convolution layer with full stride."""
        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID', name=name)


    def max_pool_2x2(self, x,name=''):
        """max_pool_2x2 downsamples a feature map by 2X."""
        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                                strides=[1, 2, 2, 1], 
                                padding='VALID',
                                name=name
                                )


    def create_loss(self):
        with tf.name_scope('loss'):
            self.loss = tf.reduce_mean(
                            tf.compat.v1.nn.softmax_cross_entropy_with_logits(labels=self.Y,logits=self.Y_predicted)
                        ) 

    def create_optimiser(self,kwargs):
        with tf.name_scope('optimiser'):
            self.optimizer = tf.compat.v1.train.GradientDescentOptimizer(**kwargs).minimize(self.loss,global_step=self.global_step) 
            #self.optimizer = tf.train.AdamOptimizer(**kwargs).minimize(self.loss,global_step=self.global_step)

    def create_accuracy(self):
        with tf.name_scope('accuracy'):
            correct_prediction = tf.equal(tf.argmax(self.Y, 1), tf.argmax(self.Y_predicted, 1))
            correct_prediction = tf.cast(correct_prediction, tf.float64)
            self.accuracy = tf.reduce_mean(correct_prediction)
