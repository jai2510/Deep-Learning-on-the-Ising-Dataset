{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "seed=12\n",
    "np.random.seed(seed)\n",
    "import sys, os, argparse\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.python.framework import dtypes\n",
    "# suppress tflow compilation warnings\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, data_X, data_Y, dtype=dtypes.float32):\n",
    "        \"\"\"Checks data and casts it into correct data type. \"\"\"\n",
    "\n",
    "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "            raise TypeError('Invalid dtype %r, expected uint8 or float32' % dtype)\n",
    "\n",
    "        assert data_X.shape[0] == data_Y.shape[0], ('data_X.shape: %s data_Y.shape: %s' % (data_X.shape, data_Y.shape))\n",
    "        self.num_examples = data_X.shape[0]\n",
    "\n",
    "        if dtype == dtypes.float32:\n",
    "            data_X = data_X.astype(np.float32)\n",
    "        self.data_X = data_X\n",
    "        self.data_Y = data_Y \n",
    "\n",
    "        self.epochs_completed = 0\n",
    "        self.index_in_epoch = 0\n",
    "\n",
    "    def next_batch(self, batch_size, seed=None):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += batch_size\n",
    "        if self.index_in_epoch > self.num_examples:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self.data_X = self.data_X[perm]\n",
    "            self.data_Y = self.data_Y[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.index_in_epoch = batch_size\n",
    "            assert batch_size <= self.num_examples\n",
    "        end = self.index_in_epoch\n",
    "\n",
    "        return self.data_X[start:end], self.data_Y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from urllib.request import urlopen \n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # path to data directory (for testing)\n",
    "    #path_to_data=os.path.expanduser('~')+'/Dropbox/MachineLearningReview/Datasets/isingMC/'\n",
    "\n",
    "    url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
    "\n",
    "    ######### LOAD DATA\n",
    "    # The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "    data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
    "    # The labels are obtained from the following file:\n",
    "    label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "\n",
    "    #DATA\n",
    "    data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "    data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "    data=data.astype('int')\n",
    "    data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "    #LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "    labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "    \n",
    "    print(\"Finished loading data\")\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def prepare_data(data, labels, dtype=dtypes.float32, test_size=0.2, validation_size=5000):\n",
    "    \n",
    "    L=40 # linear system size\n",
    "\n",
    "    # divide data into ordered, critical and disordered\n",
    "    X_ordered=data[:70000,:]\n",
    "    Y_ordered=labels[:70000]\n",
    "\n",
    "    X_critical=data[70000:100000,:]\n",
    "    Y_critical=labels[70000:100000]\n",
    "\n",
    "    X_disordered=data[100000:,:]\n",
    "    Y_disordered=labels[100000:]\n",
    "\n",
    "    # define training and test data sets\n",
    "    X=np.concatenate((X_ordered,X_disordered)) #np.concatenate((X_ordered,X_critical,X_disordered))\n",
    "    Y=np.concatenate((Y_ordered,Y_disordered)) #np.concatenate((Y_ordered,Y_critical,Y_disordered))\n",
    "\n",
    "    # pick random data points from ordered and disordered states to create the training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, train_size=1.0-test_size)\n",
    "\n",
    "    # make data categorical (i.e [0,1] or [1,0])\n",
    "    Y_train=to_categorical(Y_train)\n",
    "    Y_test=to_categorical(Y_test)\n",
    "    Y_critical=to_categorical(Y_critical)\n",
    "\n",
    "\n",
    "    if not 0 <= validation_size <= len(X_train):\n",
    "        raise ValueError('Validation size should be between 0 and {}. Received: {}.'.format(len(X_train), validation_size))\n",
    "\n",
    "    X_validation = X_train[:validation_size]\n",
    "    Y_validation = Y_train[:validation_size]\n",
    "    X_train = X_train[validation_size:]\n",
    "    Y_train = Y_train[validation_size:]\n",
    "\n",
    "    # create data sets\n",
    "    dataset = {\n",
    "        'train':DataSet(X_train, Y_train),\n",
    "        'test':DataSet(X_test, Y_test),\n",
    "        'critical':DataSet(X_critical, Y_critical),\n",
    "        'validation':DataSet(X_validation, Y_validation)\n",
    "    }\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Ising_DNN():\n",
    "    data, labels = load_data()\n",
    "    return prepare_data(data, labels, test_size=0.2, validation_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(object):\n",
    "    def __init__(self, N_neurons, opt_kwargs):\n",
    "        \"\"\"Builds the TFlow graph for the DNN.\n",
    "\n",
    "        N_neurons: number of neurons in the hidden layer\n",
    "        opt_kwargs: optimizer's arguments\n",
    "\n",
    "        \"\"\" \n",
    "\n",
    "        # define global step for checkpointing\n",
    "        self.global_step=tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.L=40 # system linear size\n",
    "        self.n_feats=self.L**2 # 40x40 square lattice\n",
    "        self.n_categories=2 # 2 Ising phases: ordered and disordered\n",
    "\n",
    "\n",
    "        # create placeholders for input X and label Y\n",
    "        self.create_placeholders()\n",
    "        # create weight and bias, initialized to 0 and construct DNN to predict Y from X\n",
    "        self.deep_layer_neurons=N_neurons\n",
    "        self.create_DNN()\n",
    "        # define loss function\n",
    "        self.create_loss()\n",
    "        # use gradient descent to minimize loss\n",
    "        self.create_optimiser(opt_kwargs)\n",
    "        #  create accuracy\n",
    "        self.create_accuracy()\n",
    "\n",
    "\n",
    "    def create_placeholders(self): \n",
    "        with tf.name_scope('data'):\n",
    "            # input layer\n",
    "            self.X=tf.compat.v1.placeholder(tf.float32, shape=(None, self.n_feats), name=\"X_data\")\n",
    "            # target\n",
    "            self.Y=tf.compat.v1.placeholder(tf.float32, shape=(None, self.n_categories), name=\"Y_data\")\n",
    "            # p\n",
    "            self.dropout_keepprob=tf.compat.v1.placeholder(tf.float32, name=\"keep_probability\")\n",
    "\n",
    "    def _weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "        # weights are drawn from a normal distribution with std 0.1 and mean 0.\n",
    "        initial = tf.random.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.compat.v1.Variable(initial, dtype=dtype, name=name)\n",
    "\n",
    "\n",
    "    def _bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "        initial = tf.compat.v1.constant(0.1, shape=shape) \n",
    "        return tf.compat.v1.Variable(initial, dtype=dtype, name=name)\n",
    "\n",
    "    \n",
    "    def create_DNN(self):\n",
    "        with tf.name_scope('DNN'):\n",
    "\n",
    "            # Fully connected layer\n",
    "            W_fc1 = self._weight_variable([self.n_feats, self.deep_layer_neurons],name='fc1',dtype=tf.float32)\n",
    "            b_fc1 = self._bias_variable([self.deep_layer_neurons],name='fc1',dtype=tf.float32)\n",
    "\n",
    "            a_fc1 = tf.nn.relu(tf.matmul(self.X, W_fc1) + b_fc1)\n",
    "\n",
    "            # Softmax layer (see loss function)\n",
    "            W_fc2 = self._weight_variable([self.deep_layer_neurons, self.n_categories],name='fc2',dtype=tf.float32)\n",
    "            b_fc2 = self._bias_variable([self.n_categories],name='fc2',dtype=tf.float32)\n",
    "        \n",
    "            self.Y_predicted = tf.matmul(a_fc1, W_fc2) + b_fc2\n",
    "\n",
    "            \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                            tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y,\n",
    "                                                                                 logits=self.Y_predicted)\n",
    "                        )\n",
    "            # no need to use tf.stop_gradient() on labels because labels are placeholders and contain no params\n",
    "            # to be optimized. Backprop will be applied only to the logits. \n",
    "\n",
    "    def create_optimiser(self,opt_kwargs):\n",
    "        with tf.name_scope('optimiser'):\n",
    "            self.optimizer = tf.compat.v1.train.GradientDescentOptimizer(**opt_kwargs).minimize(self.loss,global_step=self.global_step) \n",
    "            #self.optimizer = tf.train.AdamOptimizer(**kwargs).minimize(self.loss,global_step=self.global_step)\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y, 1), tf.argmax(self.Y_predicted, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float64) # change data type\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(neurons, lr, Ising_Data, verbose):\n",
    "    \"\"\"This function trains a DNN to solve the Ising classification problem\n",
    "\n",
    "    neurons: number of hidden neurons\n",
    "    lr: SGD learning rate\n",
    "    Ising_Data: Ising data set\n",
    "    verbose (bool): toggles output during the calculation \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    training_epochs=100\n",
    "    batch_size=100\n",
    "\n",
    "    # SGD learning params\n",
    "    opt_params=dict(learning_rate=lr)\n",
    "\n",
    "    # create DNN\n",
    "    DNN=model(neurons,opt_params)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "\n",
    "        # initialize the necessary variables, in this case, w and b\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        # train the DNN\n",
    "        for epoch in range(training_epochs): \n",
    "\n",
    "            batch_X, batch_Y = Ising_Data['train'].next_batch(batch_size,seed=seed)\n",
    "\n",
    "            loss_batch, _ = sess.run([DNN.loss,DNN.optimizer], \n",
    "                                    feed_dict={DNN.X: batch_X,\n",
    "                                               DNN.Y: batch_Y, \n",
    "                                               DNN.dropout_keepprob: 0.5} )\n",
    "            accuracy = sess.run(DNN.accuracy, \n",
    "                                feed_dict={DNN.X: batch_X,\n",
    "                                           DNN.Y: batch_Y, \n",
    "                                           DNN.dropout_keepprob: 1.0} )\n",
    "\n",
    "            # count training step\n",
    "            step = sess.run(DNN.global_step)\n",
    "\n",
    "\n",
    "        # test DNN performance on entire train test and critical data sets\n",
    "        train_loss, train_accuracy = sess.run([DNN.loss, DNN.accuracy], \n",
    "                                                    feed_dict={DNN.X: Ising_Data['train'].data_X,\n",
    "                                                               DNN.Y: Ising_Data['train'].data_Y,\n",
    "                                                               DNN.dropout_keepprob: 0.5}\n",
    "                                                                )\n",
    "        if verbose: print(\"train loss/accuracy:\", train_loss, train_accuracy)\n",
    "\n",
    "        test_loss, test_accuracy = sess.run([DNN.loss, DNN.accuracy], \n",
    "                                                    feed_dict={DNN.X: Ising_Data['test'].data_X,\n",
    "                                                               DNN.Y: Ising_Data['test'].data_Y,\n",
    "                                                               DNN.dropout_keepprob: 1.0}\n",
    "                                                               )\n",
    "\n",
    "        if verbose: print(\"test loss/accuracy:\", test_loss, test_accuracy)\n",
    "\n",
    "        critical_loss, critical_accuracy = sess.run([DNN.loss, DNN.accuracy], \n",
    "                                                    feed_dict={DNN.X: Ising_Data['critical'].data_X,\n",
    "                                                               DNN.Y: Ising_Data['critical'].data_Y,\n",
    "                                                               DNN.dropout_keepprob: 1.0}\n",
    "                                                               )\n",
    "        if verbose: print(\"crtitical loss/accuracy:\", critical_loss, critical_accuracy)\n",
    "\n",
    "\n",
    "        return train_loss,train_accuracy,test_loss,test_accuracy,critical_loss,critical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data\n"
     ]
    }
   ],
   "source": [
    "Ising_Data=prepare_Ising_DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <__main__.DataSet at 0x7ffdef565b90>,\n",
       " 'test': <__main__.DataSet at 0x7ffdea1fd8d0>,\n",
       " 'critical': <__main__.DataSet at 0x7ffdf166bd90>,\n",
       " 'validation': <__main__.DataSet at 0x7ffdf2c20e90>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ising_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss/accuracy: 0.015920106 0.9982929292929293\n",
      "test loss/accuracy: 0.014939423 0.9985384615384615\n",
      "crtitical loss/accuracy: 2.7665167 0.7263666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.015920106,\n",
       " 0.9982929292929293,\n",
       " 0.014939423,\n",
       " 0.9985384615384615,\n",
       " 2.7665167,\n",
       " 0.7263666666666667)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(neurons=1000, lr=0.01,Ising_Data=Ising_Data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
